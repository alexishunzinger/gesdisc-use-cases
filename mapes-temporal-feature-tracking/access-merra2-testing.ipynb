{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41dc66c9-dd5b-4488-a25e-e2f54425de9a",
   "metadata": {},
   "source": [
    "# Use Case: Perform in-cloud analysis with MERRA-2 and GPM IMERG\n",
    "### Originator: Brian Mapes\n",
    "### Use Case Table: https://docs.google.com/document/d/1K4N_qJs2ru2zpaqiBB4k64LJhFzrSTXZqpwF1s5IcWY/edit#\n",
    "### GitHub Repository (Mapes): https://github.com/brianmapes/VaporLakes/blob/main/TrackLakesBack_GeoPandas.py\n",
    "\n",
    "  \n",
    "  \n",
    "  \n",
    "### Author: Alexis Hunzinger\n",
    "### Date Modified: 4/22/22"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d049df60-4ffd-4819-b5da-7236f94f31b2",
   "metadata": {},
   "source": [
    "## Case 1: Using a JupyterHub in AWS us-west-2\n",
    "Steps to follow if you are beginning from a JupyterHub that is running in AWS us-west-2 (i.e. Openscapes 2i2c, GES DISC SMCE)\n",
    "\n",
    "1. Earthdata Login authentication\n",
    "2. Temporary S3 credential\n",
    "3. Identify S3 bucket link\n",
    "4. Direct S3 access of found files\n",
    "5. Extract desired variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5912078-9fa1-49ea-b2a0-46848ea0ddf1",
   "metadata": {},
   "source": [
    "### 0. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87eccfdd-7f06-4831-bc0f-e0e2c931ccdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from netrc import netrc\n",
    "from subprocess import Popen\n",
    "from platform import system\n",
    "from getpass import getpass\n",
    "from pprint import pprint\n",
    "from glob import glob\n",
    "import os\n",
    "import requests\n",
    "import xarray as xr\n",
    "import s3fs\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937c6a30-95cd-4e30-bfac-90c87e899fb7",
   "metadata": {},
   "source": [
    "### 1. Earthdata Login authentication (**SKIP THIS IF NETRC ALREADY EXISTS**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0fc9b21-af71-46f4-944b-ab5cc4b6d8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "urs = 'urs.earthdata.nasa.gov'    # Earthdata URL endpoint for authentication\n",
    "prompts = ['Enter NASA Earthdata Login Username: ',\n",
    "           'Enter NASA Earthdata Login Password: ']\n",
    "\n",
    "netrc_name = \".netrc\"\n",
    "\n",
    "# Determine if netrc file exists, and if so, if it includes NASA Earthdata Login Credentials\n",
    "try:\n",
    "    netrcDir = os.path.expanduser(f\"~/{netrc_name}\")\n",
    "    netrc(netrcDir).authenticators(urs)[0]\n",
    "\n",
    "# Below, create a netrc file and prompt user for NASA Earthdata Login Username and Password\n",
    "except FileNotFoundError:\n",
    "    homeDir = os.path.expanduser(\"~\")\n",
    "    Popen('touch {0}{2} | echo machine {1} >> {0}{2}'.format(homeDir + os.sep, urs, netrc_name), shell=True)\n",
    "    Popen('echo login {} >> {}{}'.format(getpass(prompt=prompts[0]), homeDir + os.sep, netrc_name), shell=True)\n",
    "    Popen('echo \\'password {} \\'>> {}{}'.format(getpass(prompt=prompts[1]), homeDir + os.sep, netrc_name), shell=True)\n",
    "    # Set restrictive permissions\n",
    "    Popen('chmod 0600 {0}{1}'.format(homeDir + os.sep, netrc_name), shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a913dd9a-ec05-4614-918e-616a5dde17e0",
   "metadata": {},
   "source": [
    "### 2. Temporary S3 credential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f89a42fd-1dec-4665-85a2-b82785c1c037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "s3fs.core.S3FileSystem"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gesdisc_s3 = \"https://data.gesdisc.earthdata.nasa.gov/s3credentials\"\n",
    "\n",
    "# Define a function for S3 access credentials\n",
    "\n",
    "def begin_s3_direct_access(url: str=gesdisc_s3):\n",
    "    response = requests.get(url).json()\n",
    "    return s3fs.S3FileSystem(key=response['accessKeyId'],\n",
    "                             secret=response['secretAccessKey'],\n",
    "                             token=response['sessionToken'],\n",
    "                             client_kwargs={'region_name':'us-west-2'})\n",
    "\n",
    "fs = begin_s3_direct_access()\n",
    "\n",
    "# Check that the file system is intact as an S3FileSystem object, which means that token is valid\n",
    "# Common causes of rejected S3 access tokens include incorrect passwords stored in the netrc file, or a non-existent netrc file\n",
    "type(fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f625146a-3692-4518-9fc2-4afbe276a689",
   "metadata": {},
   "source": [
    "### 3. Identify S3 bucket links\n",
    "You can find the S3 URL through a filtered Eartdata Search: https://search.earthdata.nasa.gov/search/granules/collection-details?p=C1276812863-GES_DISC&pg[0][v]=f&pg[0][gsk]=-start_date&ff=Available%20from%20AWS%20Cloud&fdc=Goddard%20Earth%20Sciences%20Data%20and%20Information%20Services%20Center%20(GES%20DISC)&tl=1648764097.138!3!!&long=0.0703125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17fe1b65-ef2b-4001-9fdd-cc2256d4bf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_merra2 = \"s3://gesdisc-cumulus-prod-protected/MERRA2/M2T1NXSLV.5.12.4/\"\n",
    "s3_imerg = \"s3://gesdisc-cumulus-prod-protected/GPM_L3/GPM_3IMERGHH.06/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4225203e-e693-4627-a167-750957ee2887",
   "metadata": {},
   "source": [
    "### 4. Identify time period and region (bounding box) of interest\n",
    "Files in the S3 bucket are organized in folders ordered by <code>YEAR/MONTH/DAILY-FILES.ext<code>\n",
    "\n",
    "\n",
    "\n",
    "For example, daily MERRA-2 files from May 2013: <code>s3://gesdisc-cumulus-prod-protected/MERRA2/M2T1NXSLV.5.12.4/2013/05/*.nc4<code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9748ad4e-d92c-45ea-bab2-c617772e1e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "year = \"2019\"\n",
    "month = \"05\"\n",
    "#day_of_year = \"015\"\n",
    "\n",
    "lat = -30,30\n",
    "lon = 30,90"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3916fa-9472-44be-889b-d7b4c5398f3a",
   "metadata": {},
   "source": [
    "### 5. List files from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3b636ed3-746d-411d-ba8b-d52ad3a1f7ef",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31,\n",
       " ['gesdisc-cumulus-prod-protected/MERRA2/M2T1NXSLV.5.12.4/2019/05/MERRA2_400.tavg1_2d_slv_Nx.20190501.nc4',\n",
       "  'gesdisc-cumulus-prod-protected/MERRA2/M2T1NXSLV.5.12.4/2019/05/MERRA2_400.tavg1_2d_slv_Nx.20190502.nc4',\n",
       "  'gesdisc-cumulus-prod-protected/MERRA2/M2T1NXSLV.5.12.4/2019/05/MERRA2_400.tavg1_2d_slv_Nx.20190503.nc4',\n",
       "  'gesdisc-cumulus-prod-protected/MERRA2/M2T1NXSLV.5.12.4/2019/05/MERRA2_400.tavg1_2d_slv_Nx.20190504.nc4',\n",
       "  'gesdisc-cumulus-prod-protected/MERRA2/M2T1NXSLV.5.12.4/2019/05/MERRA2_400.tavg1_2d_slv_Nx.20190505.nc4',\n",
       "  'gesdisc-cumulus-prod-protected/MERRA2/M2T1NXSLV.5.12.4/2019/05/MERRA2_400.tavg1_2d_slv_Nx.20190506.nc4',\n",
       "  'gesdisc-cumulus-prod-protected/MERRA2/M2T1NXSLV.5.12.4/2019/05/MERRA2_400.tavg1_2d_slv_Nx.20190507.nc4',\n",
       "  'gesdisc-cumulus-prod-protected/MERRA2/M2T1NXSLV.5.12.4/2019/05/MERRA2_400.tavg1_2d_slv_Nx.20190508.nc4',\n",
       "  'gesdisc-cumulus-prod-protected/MERRA2/M2T1NXSLV.5.12.4/2019/05/MERRA2_400.tavg1_2d_slv_Nx.20190509.nc4',\n",
       "  'gesdisc-cumulus-prod-protected/MERRA2/M2T1NXSLV.5.12.4/2019/05/MERRA2_400.tavg1_2d_slv_Nx.20190510.nc4',\n",
       "  'gesdisc-cumulus-prod-protected/MERRA2/M2T1NXSLV.5.12.4/2019/05/MERRA2_400.tavg1_2d_slv_Nx.20190511.nc4',\n",
       "  'gesdisc-cumulus-prod-protected/MERRA2/M2T1NXSLV.5.12.4/2019/05/MERRA2_400.tavg1_2d_slv_Nx.20190512.nc4',\n",
       "  'gesdisc-cumulus-prod-protected/MERRA2/M2T1NXSLV.5.12.4/2019/05/MERRA2_400.tavg1_2d_slv_Nx.20190513.nc4',\n",
       "  'gesdisc-cumulus-prod-protected/MERRA2/M2T1NXSLV.5.12.4/2019/05/MERRA2_400.tavg1_2d_slv_Nx.20190514.nc4',\n",
       "  'gesdisc-cumulus-prod-protected/MERRA2/M2T1NXSLV.5.12.4/2019/05/MERRA2_400.tavg1_2d_slv_Nx.20190515.nc4',\n",
       "  'gesdisc-cumulus-prod-protected/MERRA2/M2T1NXSLV.5.12.4/2019/05/MERRA2_400.tavg1_2d_slv_Nx.20190516.nc4',\n",
       "  'gesdisc-cumulus-prod-protected/MERRA2/M2T1NXSLV.5.12.4/2019/05/MERRA2_400.tavg1_2d_slv_Nx.20190517.nc4',\n",
       "  'gesdisc-cumulus-prod-protected/MERRA2/M2T1NXSLV.5.12.4/2019/05/MERRA2_400.tavg1_2d_slv_Nx.20190518.nc4',\n",
       "  'gesdisc-cumulus-prod-protected/MERRA2/M2T1NXSLV.5.12.4/2019/05/MERRA2_400.tavg1_2d_slv_Nx.20190519.nc4',\n",
       "  'gesdisc-cumulus-prod-protected/MERRA2/M2T1NXSLV.5.12.4/2019/05/MERRA2_400.tavg1_2d_slv_Nx.20190520.nc4',\n",
       "  'gesdisc-cumulus-prod-protected/MERRA2/M2T1NXSLV.5.12.4/2019/05/MERRA2_400.tavg1_2d_slv_Nx.20190521.nc4',\n",
       "  'gesdisc-cumulus-prod-protected/MERRA2/M2T1NXSLV.5.12.4/2019/05/MERRA2_400.tavg1_2d_slv_Nx.20190522.nc4',\n",
       "  'gesdisc-cumulus-prod-protected/MERRA2/M2T1NXSLV.5.12.4/2019/05/MERRA2_400.tavg1_2d_slv_Nx.20190523.nc4',\n",
       "  'gesdisc-cumulus-prod-protected/MERRA2/M2T1NXSLV.5.12.4/2019/05/MERRA2_400.tavg1_2d_slv_Nx.20190524.nc4',\n",
       "  'gesdisc-cumulus-prod-protected/MERRA2/M2T1NXSLV.5.12.4/2019/05/MERRA2_400.tavg1_2d_slv_Nx.20190525.nc4',\n",
       "  'gesdisc-cumulus-prod-protected/MERRA2/M2T1NXSLV.5.12.4/2019/05/MERRA2_400.tavg1_2d_slv_Nx.20190526.nc4',\n",
       "  'gesdisc-cumulus-prod-protected/MERRA2/M2T1NXSLV.5.12.4/2019/05/MERRA2_400.tavg1_2d_slv_Nx.20190527.nc4',\n",
       "  'gesdisc-cumulus-prod-protected/MERRA2/M2T1NXSLV.5.12.4/2019/05/MERRA2_400.tavg1_2d_slv_Nx.20190528.nc4',\n",
       "  'gesdisc-cumulus-prod-protected/MERRA2/M2T1NXSLV.5.12.4/2019/05/MERRA2_400.tavg1_2d_slv_Nx.20190529.nc4',\n",
       "  'gesdisc-cumulus-prod-protected/MERRA2/M2T1NXSLV.5.12.4/2019/05/MERRA2_400.tavg1_2d_slv_Nx.20190530.nc4',\n",
       "  'gesdisc-cumulus-prod-protected/MERRA2/M2T1NXSLV.5.12.4/2019/05/MERRA2_400.tavg1_2d_slv_Nx.20190531.nc4'])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3files = fs.glob(s3_merra2+\n",
    "                    year+\n",
    "                    \"/\"+\n",
    "                    month+\n",
    "                    \"/\"+\n",
    "                    \"*\")\n",
    "len(s3files), s3files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7cb7be4e-ac9a-46aa-92c5-1256c0dcc471",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, [])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # IMERG\n",
    "# s3files = fs.glob(s3_imerg+\n",
    "#                     year+\n",
    "#                     \"/\"+\n",
    "#                     month+\n",
    "#                     \"/\"+\n",
    "#                     \"*\")\n",
    "# len(s3files), s3files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4778b626-f058-4d67-ae47-365ff29e3e41",
   "metadata": {},
   "source": [
    "# ***Begin testing access methods and documenting issues/failures/speed test results***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778cee1d-c8b1-483f-9066-2123437dc589",
   "metadata": {},
   "source": [
    "### 6a. Open list of S3 files with xarray\n",
    "\n",
    "- combine='by_coords'\n",
    "- mask_and_scale=True\n",
    "- decode_cf=True\n",
    "- ***parallel=False***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1648e55c-ccc1-4ce9-b761-7d9331c9690b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using xarray's open_mfdataset to open 31 netCDF files from S3 storage (parallel=False)\n",
      "CPU times: user 31.1 s, sys: 1.03 s, total: 32.2 s\n",
      "Wall time: 41.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "merraDataset = xr.open_mfdataset(\n",
    "    paths=[fs.open(f) for f in s3files],\n",
    "    combine='by_coords',\n",
    "    mask_and_scale=True,\n",
    "    decode_cf=True,\n",
    "    parallel=False,\n",
    "#     chunks={'lat': 60,   # These were chosen arbitrarily. You must specify \n",
    "#             'lon': 120, # chunking that is suitable to the data and target\n",
    "#             'time': 100}      # analysis.\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Using xarray's open_mfdataset to open 31 netCDF files from S3 storage (parallel=False)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56c71f5-22cf-4527-8961-94a805fe87fc",
   "metadata": {},
   "source": [
    "### 6b. Open list of S3 files with xarray\n",
    "\n",
    "- combine='by_coords'\n",
    "- mask_and_scale=True\n",
    "- decode_cf=True\n",
    "- ***parallel=True***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dabcb6da-329e-441e-a513-aea855569e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using xarray's open_mfdataset to open 31 netCDF files from S3 storage (parallel=True)\n",
      "CPU times: user 31.6 s, sys: 1.01 s, total: 32.6 s\n",
      "Wall time: 40 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "merraDataset = xr.open_mfdataset(\n",
    "    paths=[fs.open(f) for f in s3files],\n",
    "    combine='by_coords',\n",
    "    mask_and_scale=True,\n",
    "    decode_cf=True,\n",
    "    parallel=True,\n",
    "#     chunks={'lat': 60,   # These were chosen arbitrarily. You must specify \n",
    "#             'lon': 120, # chunking that is suitable to the data and target\n",
    "#             'time': 100}      # analysis.\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Using xarray's open_mfdataset to open 31 netCDF files from S3 storage (parallel=True)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c33fbd-9c1e-4115-aaa8-c5c5876e468a",
   "metadata": {},
   "source": [
    "### 6c. Open list of S3 files with xarray\n",
    "\n",
    "- combine='by_coords'\n",
    "- mask_and_scale=True\n",
    "- decode_cf=True\n",
    "- parallel=False\n",
    "- ***decode_times=True***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce4e3b66-0853-4528-b455-5a5b8d79e6ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using xarray's open_mfdataset to open 31 netCDF files from S3 storage (decode_times=True)\n",
      "CPU times: user 31.7 s, sys: 1.2 s, total: 32.9 s\n",
      "Wall time: 39.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "merraDataset = xr.open_mfdataset(\n",
    "    paths=[fs.open(f) for f in s3files],\n",
    "    combine='by_coords',\n",
    "    mask_and_scale=True,\n",
    "    decode_cf=True,\n",
    "    parallel=False,\n",
    "    decode_times=True\n",
    "#     chunks={'lat': 60,   # These were chosen arbitrarily. You must specify \n",
    "#             'lon': 120, # chunking that is suitable to the data and target\n",
    "#             'time': 100}      # analysis.\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Using xarray's open_mfdataset to open 31 netCDF files from S3 storage (decode_times=True)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068234e3-ab3f-4f5d-8826-fa8101be3a3e",
   "metadata": {},
   "source": [
    "### 6d. Open list of S3 files with xarray\n",
    "\n",
    "- combine='by_coords'\n",
    "- mask_and_scale=True\n",
    "- decode_cf=True\n",
    "- parallel=False\n",
    "- ***decode_times=False***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "199ea8de-2b57-4e12-af5a-1aa10b7d3433",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Could not find any dimension coordinates to use to order the datasets for concatenation",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:1\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/xarray/backends/api.py:936\u001b[0m, in \u001b[0;36mopen_mfdataset\u001b[0;34m(paths, chunks, concat_dim, compat, preprocess, engine, data_vars, coords, combine, parallel, join, attrs_file, combine_attrs, **kwargs)\u001b[0m\n\u001b[1;32m    923\u001b[0m     combined \u001b[38;5;241m=\u001b[39m _nested_combine(\n\u001b[1;32m    924\u001b[0m         datasets,\n\u001b[1;32m    925\u001b[0m         concat_dims\u001b[38;5;241m=\u001b[39mconcat_dim,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    931\u001b[0m         combine_attrs\u001b[38;5;241m=\u001b[39mcombine_attrs,\n\u001b[1;32m    932\u001b[0m     )\n\u001b[1;32m    933\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m combine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mby_coords\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    934\u001b[0m     \u001b[38;5;66;03m# Redo ordering from coordinates, ignoring how they were ordered\u001b[39;00m\n\u001b[1;32m    935\u001b[0m     \u001b[38;5;66;03m# previously\u001b[39;00m\n\u001b[0;32m--> 936\u001b[0m     combined \u001b[38;5;241m=\u001b[39m \u001b[43mcombine_by_coords\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    937\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    938\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    939\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_vars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_vars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    940\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcoords\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoords\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcombine_attrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcombine_attrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    945\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    946\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m is an invalid option for the keyword argument\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    947\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m ``combine``\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(combine)\n\u001b[1;32m    948\u001b[0m     )\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/xarray/core/combine.py:975\u001b[0m, in \u001b[0;36mcombine_by_coords\u001b[0;34m(data_objects, compat, data_vars, coords, fill_value, join, combine_attrs, datasets)\u001b[0m\n\u001b[1;32m    973\u001b[0m     concatenated_grouped_by_data_vars \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    974\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mvars\u001b[39m, datasets_with_same_vars \u001b[38;5;129;01min\u001b[39;00m grouped_by_vars:\n\u001b[0;32m--> 975\u001b[0m         concatenated \u001b[38;5;241m=\u001b[39m \u001b[43m_combine_single_variable_hypercube\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdatasets_with_same_vars\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata_vars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_vars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcoords\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoords\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[43m            \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcombine_attrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcombine_attrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    984\u001b[0m         concatenated_grouped_by_data_vars\u001b[38;5;241m.\u001b[39mappend(concatenated)\n\u001b[1;32m    986\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m merge(\n\u001b[1;32m    987\u001b[0m     concatenated_grouped_by_data_vars,\n\u001b[1;32m    988\u001b[0m     compat\u001b[38;5;241m=\u001b[39mcompat,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    991\u001b[0m     combine_attrs\u001b[38;5;241m=\u001b[39mcombine_attrs,\n\u001b[1;32m    992\u001b[0m )\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/xarray/core/combine.py:626\u001b[0m, in \u001b[0;36m_combine_single_variable_hypercube\u001b[0;34m(datasets, fill_value, data_vars, coords, compat, join, combine_attrs)\u001b[0m\n\u001b[1;32m    620\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(datasets) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    621\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    622\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAt least one Dataset is required to resolve variable names \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    623\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor combined hypercube.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    624\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m combined_ids, concat_dims \u001b[38;5;241m=\u001b[39m \u001b[43m_infer_concat_order_from_coords\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fill_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# check that datasets form complete hypercube\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     _check_shape_tile_ids(combined_ids)\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/xarray/core/combine.py:144\u001b[0m, in \u001b[0;36m_infer_concat_order_from_coords\u001b[0;34m(datasets)\u001b[0m\n\u001b[1;32m    139\u001b[0m             tile_ids \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    140\u001b[0m                 tile_id \u001b[38;5;241m+\u001b[39m (position,) \u001b[38;5;28;01mfor\u001b[39;00m tile_id, position \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(tile_ids, order)\n\u001b[1;32m    141\u001b[0m             ]\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(datasets) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m concat_dims:\n\u001b[0;32m--> 144\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not find any dimension coordinates to use to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    146\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morder the datasets for concatenation\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    147\u001b[0m     )\n\u001b[1;32m    149\u001b[0m combined_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(tile_ids, datasets))\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m combined_ids, concat_dims\n",
      "\u001b[0;31mValueError\u001b[0m: Could not find any dimension coordinates to use to order the datasets for concatenation"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "merraDataset = xr.open_mfdataset(\n",
    "    paths=[fs.open(f) for f in s3files],\n",
    "    combine='by_coords',\n",
    "    mask_and_scale=True,\n",
    "    decode_cf=True,\n",
    "    parallel=False,\n",
    "    decode_times=False,\n",
    "#     chunks={'lat': 60,   # These were chosen arbitrarily. You must specify \n",
    "#             'lon': 120, # chunking that is suitable to the data and target\n",
    "#             'time': 100}      # analysis.\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Using xarray's open_mfdataset to open 31 netCDF files from S3 storage (decode_times=False)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17886fb7-e665-4022-80eb-b6be38cea5b2",
   "metadata": {},
   "source": [
    "### 6e. Open list of S3 files with xarray\n",
    "\n",
    "- combine='by_coords'\n",
    "- mask_and_scale=True\n",
    "- decode_cf=True\n",
    "- parallel=False\n",
    "- decode_times=True\n",
    "- ***chunks={'lat':10,'lon':10,'time':100}***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ad67edbd-4dd3-493c-841e-430b61a79b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using xarray's open_mfdataset to open 31 netCDF files from S3 storage (chunks specified)\n",
      "CPU times: user 45.5 s, sys: 3.83 s, total: 49.4 s\n",
      "Wall time: 1min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "merraDataset1 = xr.open_mfdataset(\n",
    "    paths=[fs.open(f) for f in s3files],\n",
    "    combine='by_coords',\n",
    "    mask_and_scale=True,\n",
    "    decode_cf=True,\n",
    "    parallel=False,\n",
    "    decode_times=True,\n",
    "    chunks={'lat': 10,   # These were chosen arbitrarily. You must specify \n",
    "            'lon': 10, # chunking that is suitable to the data and target\n",
    "            'time': 100}      # analysis.\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Using xarray's open_mfdataset to open 31 netCDF files from S3 storage (chunks specified)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4644f9e2-d686-4d6c-a3ac-4f5f996e3bbe",
   "metadata": {},
   "source": [
    "### 6f. Open list of S3 files with xarray\n",
    "\n",
    "- combine='by_coords'\n",
    "- mask_and_scale=True\n",
    "- decode_cf=True\n",
    "- parallel=False\n",
    "- decode_times=True\n",
    "- ***chunks={'lat':30,'lon':30,'time':100}***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "147bbb6d-e643-4c84-9f58-90007614779d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using xarray's open_mfdataset to open 31 netCDF files from S3 storage (see chunks specified)\n",
      "CPU times: user 32 s, sys: 848 ms, total: 32.8 s\n",
      "Wall time: 41.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "merraDataset1 = xr.open_mfdataset(\n",
    "    paths=[fs.open(f) for f in s3files],\n",
    "    combine='by_coords',\n",
    "    mask_and_scale=True,\n",
    "    decode_cf=True,\n",
    "    parallel=False,\n",
    "    decode_times=True,\n",
    "    chunks={'lat': 30,   # These were chosen arbitrarily. You must specify \n",
    "            'lon': 30, # chunking that is suitable to the data and target\n",
    "            'time': 50}      # analysis.\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Using xarray's open_mfdataset to open 31 netCDF files from S3 storage (see chunks specified)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f14688-5716-424e-b51e-f54095f5b78d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b99cf36-9280-4923-b509-b86c92e26509",
   "metadata": {},
   "source": [
    "### 6f. Open a single S3 file (.nc4) with xarray's open_zarr()\n",
    "\n",
    "- Method attempt pulled from: https://medium.com/pangeo/cloud-performant-reading-of-netcdf4-hdf5-data-using-the-zarr-library-1a95c5c92314"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dde02839-2647-425f-92eb-37278334c7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fsspec as fsspec\n",
    "f = s3files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "09456a78-451a-46dc-b485-8b9e042be7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<OpenFile '/home/jovyan/gesdisc-cumulus-prod-protected/MERRA2/M2T1NXSLV.5.12.4/2019/05/MERRA2_400.tavg1_2d_slv_Nx.20190501.nc4'>\n",
      "CPU times: user 406 µs, sys: 0 ns, total: 406 µs\n",
      "Wall time: 350 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ncfile = fsspec.open(f)\n",
    "print(ncfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "192321fc-e326-451a-93da-633a9c69f51f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/jovyan/gesdisc-cumulus-prod-protected/MERRA2/M2T1NXSLV.5.12.4/2019/05/MERRA2_400.tavg1_2d_slv_Nx.20190501.nc4'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:2\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/xarray/backends/zarr.py:769\u001b[0m, in \u001b[0;36mopen_zarr\u001b[0;34m(store, group, synchronizer, chunks, decode_cf, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, consolidated, overwrite_encoded_chunks, chunk_store, storage_options, decode_timedelta, use_cftime, **kwargs)\u001b[0m\n\u001b[1;32m    756\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    757\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopen_zarr() got unexpected keyword arguments \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(kwargs\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m    758\u001b[0m     )\n\u001b[1;32m    760\u001b[0m backend_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    761\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msynchronizer\u001b[39m\u001b[38;5;124m\"\u001b[39m: synchronizer,\n\u001b[1;32m    762\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconsolidated\u001b[39m\u001b[38;5;124m\"\u001b[39m: consolidated,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    766\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstacklevel\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m4\u001b[39m,\n\u001b[1;32m    767\u001b[0m }\n\u001b[0;32m--> 769\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    772\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_cf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_cf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    773\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask_and_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask_and_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    774\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_times\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_times\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    775\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconcat_characters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcat_characters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    776\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_coords\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_coords\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    777\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mzarr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    778\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    779\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdrop_variables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_variables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    780\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbackend_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackend_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_timedelta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_timedelta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cftime\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cftime\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    783\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    784\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/xarray/backends/api.py:495\u001b[0m, in \u001b[0;36mopen_dataset\u001b[0;34m(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, backend_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    483\u001b[0m decoders \u001b[38;5;241m=\u001b[39m _resolve_decoders_kwargs(\n\u001b[1;32m    484\u001b[0m     decode_cf,\n\u001b[1;32m    485\u001b[0m     open_backend_dataset_parameters\u001b[38;5;241m=\u001b[39mbackend\u001b[38;5;241m.\u001b[39mopen_dataset_parameters,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    491\u001b[0m     decode_coords\u001b[38;5;241m=\u001b[39mdecode_coords,\n\u001b[1;32m    492\u001b[0m )\n\u001b[1;32m    494\u001b[0m overwrite_encoded_chunks \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite_encoded_chunks\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 495\u001b[0m backend_ds \u001b[38;5;241m=\u001b[39m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdrop_variables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_variables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdecoders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    501\u001b[0m ds \u001b[38;5;241m=\u001b[39m _dataset_from_backend_dataset(\n\u001b[1;32m    502\u001b[0m     backend_ds,\n\u001b[1;32m    503\u001b[0m     filename_or_obj,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    511\u001b[0m )\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/xarray/backends/zarr.py:816\u001b[0m, in \u001b[0;36mZarrBackendEntrypoint.open_dataset\u001b[0;34m(self, filename_or_obj, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, use_cftime, decode_timedelta, group, mode, synchronizer, consolidated, chunk_store, storage_options, stacklevel)\u001b[0m\n\u001b[1;32m    797\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopen_dataset\u001b[39m(\n\u001b[1;32m    798\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    799\u001b[0m     filename_or_obj,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    813\u001b[0m     stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m    814\u001b[0m ):\n\u001b[0;32m--> 816\u001b[0m     filename_or_obj \u001b[38;5;241m=\u001b[39m \u001b[43m_normalize_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    817\u001b[0m     store \u001b[38;5;241m=\u001b[39m ZarrStore\u001b[38;5;241m.\u001b[39mopen_group(\n\u001b[1;32m    818\u001b[0m         filename_or_obj,\n\u001b[1;32m    819\u001b[0m         group\u001b[38;5;241m=\u001b[39mgroup,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    826\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    827\u001b[0m     )\n\u001b[1;32m    829\u001b[0m     store_entrypoint \u001b[38;5;241m=\u001b[39m StoreBackendEntrypoint()\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/xarray/backends/common.py:23\u001b[0m, in \u001b[0;36m_normalize_path\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_normalize_path\u001b[39m(path):\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, os\u001b[38;5;241m.\u001b[39mPathLike):\n\u001b[0;32m---> 23\u001b[0m         path \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_remote_uri(path):\n\u001b[1;32m     26\u001b[0m         path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexpanduser(path))\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/fsspec/core.py:98\u001b[0m, in \u001b[0;36mOpenFile.__fspath__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__fspath__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;66;03m# may raise if cannot be resolved to local file\u001b[39;00m\n\u001b[0;32m---> 98\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__fspath__()\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/fsspec/core.py:140\u001b[0m, in \u001b[0;36mOpenFile.open\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;124;03m\"\"\"Materialise this as a real open file without context\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m    The file should be explicitly closed to avoid enclosed file\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;124;03m    been deleted; but a with-context is better style.\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 140\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__enter__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     closer \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mclose\n\u001b[1;32m    142\u001b[0m     fobjects \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfobjects\u001b[38;5;241m.\u001b[39mcopy()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/fsspec/core.py:103\u001b[0m, in \u001b[0;36mOpenFile.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    101\u001b[0m     mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 103\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfobjects \u001b[38;5;241m=\u001b[39m [f]\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/fsspec/spec.py:1030\u001b[0m, in \u001b[0;36mAbstractFileSystem.open\u001b[0;34m(self, path, mode, block_size, cache_options, compression, **kwargs)\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1029\u001b[0m     ac \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mautocommit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_intrans)\n\u001b[0;32m-> 1030\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1032\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1033\u001b[0m \u001b[43m        \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1034\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautocommit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mac\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1035\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1036\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1037\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1038\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1039\u001b[0m         \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfsspec\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompression\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compr\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/fsspec/implementations/local.py:155\u001b[0m, in \u001b[0;36mLocalFileSystem._open\u001b[0;34m(self, path, mode, block_size, **kwargs)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_mkdir \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent(path), exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mLocalFileOpener\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/fsspec/implementations/local.py:250\u001b[0m, in \u001b[0;36mLocalFileOpener.__init__\u001b[0;34m(self, path, mode, autocommit, fs, compression, **kwargs)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression \u001b[38;5;241m=\u001b[39m get_compression(path, compression)\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocksize \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mDEFAULT_BUFFER_SIZE\n\u001b[0;32m--> 250\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/fsspec/implementations/local.py:255\u001b[0m, in \u001b[0;36mLocalFileOpener._open\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf\u001b[38;5;241m.\u001b[39mclosed:\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautocommit \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m--> 255\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression:\n\u001b[1;32m    257\u001b[0m             compress \u001b[38;5;241m=\u001b[39m compr[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression]\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/jovyan/gesdisc-cumulus-prod-protected/MERRA2/M2T1NXSLV.5.12.4/2019/05/MERRA2_400.tavg1_2d_slv_Nx.20190501.nc4'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#skipping a chunk store step\n",
    "ds = xr.open_zarr(ncfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00570d12-cf4b-47ba-96fc-fe3784d15cc6",
   "metadata": {},
   "source": [
    "### 6g. Open a single S3 file (.nc4) with zarr-eosdis-store\n",
    "\n",
    "- Method attempt pulled from: https://github.com/nasa/zarr-eosdis-store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8b3662f4-b0aa-427c-9304-d007393e3197",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eosdis_store import EosdisStore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fe2be1-01f9-49de-8c37-0de0b72b6c7f",
   "metadata": {},
   "source": [
    "### 6h. Open a single S3 file (.nc4) using kerchunk json to mimic zarr\n",
    "\n",
    "- Method adapted for NASA data by Aaron Friesz: https://github.com/NASA-Openscapes/earthdata-cloud-cookbook/blob/ornl_daymet_access/examples/GESDISC/GESDISC_MERRA2_tavg1_2d_flx_Nx__Kerchunk.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8fb91c-aa64-4fae-b03b-13fb11f47813",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
